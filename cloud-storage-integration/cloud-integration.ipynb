{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from aws_snowflake_config import aws_s3_connection, snowflake_connection\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client, bucket_name = aws_s3_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the files with the extension in the directory_path\n",
    "def get_files_in_directory(directory_path, extension):\n",
    "    files=[]\n",
    "    for f in os.listdir(directory_path):\n",
    "        if f.endswith(extension):\n",
    "            files.append(f)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CSV_Data files:\n",
      "['scraped_data.csv']\n",
      "File uploaded successfully: ../web-scraping-and-dataset/scraped_data.csv -> s3://bigdata-group3-assignment2/CSV_Data/scraped_data.csv\n",
      "\n",
      " PyPDF files:\n",
      "['PyPDF_RR_2024_l1_combined.txt', 'PyPDF_RR_2024_l2_combined.txt', 'PyPDF_RR_2024_l3_combined.txt']\n",
      "File uploaded successfully: ../pdf-extractions/pypdf\\PyPDF_RR_2024_l1_combined.txt -> s3://bigdata-group3-assignment2/PyPDF/PyPDF_RR_2024_l1_combined.txt\n",
      "File uploaded successfully: ../pdf-extractions/pypdf\\PyPDF_RR_2024_l2_combined.txt -> s3://bigdata-group3-assignment2/PyPDF/PyPDF_RR_2024_l2_combined.txt\n",
      "File uploaded successfully: ../pdf-extractions/pypdf\\PyPDF_RR_2024_l3_combined.txt -> s3://bigdata-group3-assignment2/PyPDF/PyPDF_RR_2024_l3_combined.txt\n",
      "\n",
      " Grobid files:\n",
      "['Grobid_RR_2024_l1_combined.txt', 'Grobid_RR_2024_l2_combined.txt', 'Grobid_RR_2024_l3_combined.txt']\n",
      "File uploaded successfully: ../pdf-extractions/grobid\\Grobid_RR_2024_l1_combined.txt -> s3://bigdata-group3-assignment2/Grobid/Grobid_RR_2024_l1_combined.txt\n",
      "File uploaded successfully: ../pdf-extractions/grobid\\Grobid_RR_2024_l2_combined.txt -> s3://bigdata-group3-assignment2/Grobid/Grobid_RR_2024_l2_combined.txt\n",
      "File uploaded successfully: ../pdf-extractions/grobid\\Grobid_RR_2024_l3_combined.txt -> s3://bigdata-group3-assignment2/Grobid/Grobid_RR_2024_l3_combined.txt\n"
     ]
    }
   ],
   "source": [
    "# Function  to upload the file of type extension to S3 bucket\n",
    "def upload_file_to_s3(file_path, type, extension):\n",
    "    try:\n",
    "        # Fetching all files in the directory with the given extension\n",
    "        list_of_files = get_files_in_directory(file_path,extension)\n",
    "        print(\"\\n\",type.rstrip(\"/\"),\"files:\",)\n",
    "        print(list_of_files)\n",
    "        \n",
    "        # Upload if files exist\n",
    "        if list_of_files:\n",
    "            for file in list_of_files:\n",
    "                try:\n",
    "                    file_full_path=os.path.join(file_path, file)    # file path in local directory\n",
    "                    key_value = type + file   # file path in S3 bucket\n",
    "                    \n",
    "                    # Upload the file to S3 using boto3\n",
    "                    response = s3_client.upload_file(file_full_path, bucket_name, key_value)\n",
    "                    print(f'File uploaded successfully: {file_full_path} -> s3://{bucket_name}/{key_value}')\n",
    "                except ClientError as e:\n",
    "                    logging.error(e)\n",
    "        else:\n",
    "            print(f'No files found in the {file_path} directory.')\n",
    "    except Exception as e:\n",
    "        print(f'Error uploading file {file_path}: {e}')\n",
    "\n",
    "upload_file_to_s3('../web-scraping-and-dataset/','CSV_Data/','.csv')\n",
    "upload_file_to_s3('../pdf-extractions/pypdf','PyPDF/','.txt')\n",
    "upload_file_to_s3('../pdf-extractions/grobid','Grobid/','.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading metadata of text files to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import datetime as dt\n",
    "import mimetypes\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        file_name        type        creation_date   size  \\\n",
      "0  Grobid_RR_2024_l1_combined.txt  text/plain  2024-02-15 19:23:24  43570   \n",
      "1  Grobid_RR_2024_l2_combined.txt  text/plain  2024-02-15 19:23:24  44561   \n",
      "2  Grobid_RR_2024_l3_combined.txt  text/plain  2024-02-15 19:23:24  28088   \n",
      "\n",
      "                                              s3_url  \n",
      "0  https://bigdata-group3-assignment2.s3.amazonaw...  \n",
      "1  https://bigdata-group3-assignment2.s3.amazonaw...  \n",
      "2  https://bigdata-group3-assignment2.s3.amazonaw...  \n"
     ]
    }
   ],
   "source": [
    "# Getting metadata of the text files\n",
    "\n",
    "def getting_metadata(path_to_txt_file):\n",
    "    files_metadata_df=pd.DataFrame(columns=['file_name','type', 'creation_date','size','s3_url'])\n",
    "    files=get_files_in_directory(path_to_txt_file, '.txt')\n",
    "    for f in files:\n",
    "        file_full_path=os.path.join(path_to_txt_file, f)\n",
    "        file_stats = os.stat(file_full_path)\n",
    "        \n",
    "        file_type = mimetypes.guess_type(file_full_path)[0]     #file type\n",
    "        if not file_type:\n",
    "            file_type = 'Unknown'\n",
    "        f_creation_date= file_stats.st_ctime    # file creation time\n",
    "        f_size = file_stats.st_size     # file size in bytes\n",
    "        f_url = f\"https://{bucket_name}.s3.amazonaws.com/Grobid/{f}\" # file url\n",
    "        \n",
    "        files_metadata_df.loc[len(files_metadata_df)]=[f, file_type, dt.datetime.fromtimestamp(int(f_creation_date)).strftime('%Y-%m-%d %H:%M:%S'), f_size, f_url]\n",
    "        \n",
    "    return files_metadata_df\n",
    "    \n",
    "f_metadata = getting_metadata('../pdf-extractions/grobid')\n",
    "print(f_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to csv\n",
    "f_metadata.to_csv('grobid_text_metadata.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Connected to Snowflake\n"
     ]
    }
   ],
   "source": [
    "# Creating SQLAlchemy connection for snowflake\n",
    "user, password, account, warehouse, database, schema = snowflake_connection()\n",
    "sql_engine = create_engine(f'snowflake://{user}:{password}@{account}/?warehouse={warehouse}&database={database}&schema={schema}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_internal_stage(engine, stage_name):\n",
    "    create_stage_query = f\"\"\"\n",
    "    CREATE STAGE IF NOT EXISTS {stage_name};\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(create_stage_query))\n",
    "\n",
    "create_internal_stage(sql_engine, 'grobid_metadata_stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_data_into_stage(engine, csv_file_path, stage_name):\n",
    "    put_data_query = f\"\"\"\n",
    "    PUT file://{csv_file_path} @{stage_name};\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(put_data_query))\n",
    "\n",
    "put_data_into_stage(sql_engine, './grobid_text_metadata.csv', 'grobid_metadata_stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_with_csv_structure(engine, table_name):\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {table_name} (\n",
    "        file_name STRING,\n",
    "        type STRING,\n",
    "        creation_date DATETIME,\n",
    "        size NUMBER,\n",
    "        s3_url STRING\n",
    "    );\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(create_table_query))\n",
    "        \n",
    "create_table_with_csv_structure(sql_engine, 'grobid_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_stage_to_table(engine, table_name, stage_name):\n",
    "    copy_into_query = f\"\"\"\n",
    "    COPY INTO {table_name} FROM @{stage_name} FILE_FORMAT = (TYPE = CSV SKIP_HEADER = 1) ON_ERROR = 'CONTINUE';\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(copy_into_query))\n",
    "        \n",
    "load_data_from_stage_to_table(sql_engine, 'grobid_metadata','grobid_metadata_stage')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
