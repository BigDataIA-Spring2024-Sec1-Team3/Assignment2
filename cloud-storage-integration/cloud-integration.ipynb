{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from aws_snowflake_config import aws_s3_connection, snowflake_connection\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client, bucket_name = aws_s3_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the files with the extension in the directory_path\n",
    "def get_files_in_directory(directory_path, extension):\n",
    "    files=[]\n",
    "    for f in os.listdir(directory_path):\n",
    "        if f.endswith(extension):\n",
    "            files.append(f)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CSV_Data files:\n",
      "['scraped_data.csv']\n",
      "File uploaded successfully: ../web-scraping-and-dataset/scraped_data.csv -> s3://bigdata-group3-assignment2/CSV_Data/scraped_data.csv\n",
      "\n",
      " PyPDF files:\n",
      "['PyPDF_RR_2024_l1_combined.txt', 'PyPDF_RR_2024_l2_combined.txt', 'PyPDF_RR_2024_l3_combined.txt']\n",
      "File uploaded successfully: ../pdf-extractions/pypdf\\PyPDF_RR_2024_l1_combined.txt -> s3://bigdata-group3-assignment2/PyPDF/PyPDF_RR_2024_l1_combined.txt\n",
      "File uploaded successfully: ../pdf-extractions/pypdf\\PyPDF_RR_2024_l2_combined.txt -> s3://bigdata-group3-assignment2/PyPDF/PyPDF_RR_2024_l2_combined.txt\n",
      "File uploaded successfully: ../pdf-extractions/pypdf\\PyPDF_RR_2024_l3_combined.txt -> s3://bigdata-group3-assignment2/PyPDF/PyPDF_RR_2024_l3_combined.txt\n",
      "\n",
      " Grobid files:\n",
      "['Grobid_RR_2024_l1_combined.txt', 'Grobid_RR_2024_l2_combined.txt', 'Grobid_RR_2024_l3_combined.txt']\n",
      "File uploaded successfully: ../pdf-extractions/grobid\\Grobid_RR_2024_l1_combined.txt -> s3://bigdata-group3-assignment2/Grobid/Grobid_RR_2024_l1_combined.txt\n",
      "File uploaded successfully: ../pdf-extractions/grobid\\Grobid_RR_2024_l2_combined.txt -> s3://bigdata-group3-assignment2/Grobid/Grobid_RR_2024_l2_combined.txt\n",
      "File uploaded successfully: ../pdf-extractions/grobid\\Grobid_RR_2024_l3_combined.txt -> s3://bigdata-group3-assignment2/Grobid/Grobid_RR_2024_l3_combined.txt\n"
     ]
    }
   ],
   "source": [
    "# Function  to upload the file of type extension to S3 bucket\n",
    "def upload_file_to_s3(file_path, type, extension):\n",
    "    try:\n",
    "        # Fetching all files in the directory with the given extension\n",
    "        list_of_files = get_files_in_directory(file_path,extension)\n",
    "        print(\"\\n\",type.rstrip(\"/\"),\"files:\",)\n",
    "        print(list_of_files)\n",
    "        \n",
    "        # Upload if files exist\n",
    "        if list_of_files:\n",
    "            for file in list_of_files:\n",
    "                try:\n",
    "                    file_full_path=os.path.join(file_path, file)    # file path in local directory\n",
    "                    key_value = type + file   # file path in S3 bucket\n",
    "                    \n",
    "                    # Upload the file to S3 using boto3\n",
    "                    response = s3_client.upload_file(file_full_path, bucket_name, key_value)\n",
    "                    print(f'File uploaded successfully: {file_full_path} -> s3://{bucket_name}/{key_value}')\n",
    "                except ClientError as e:\n",
    "                    logging.error(e)\n",
    "        else:\n",
    "            print(f'No files found in the {file_path} directory.')\n",
    "    except Exception as e:\n",
    "        print(f'Error uploading file {file_path}: {e}')\n",
    "\n",
    "upload_file_to_s3('../web-scraping-and-dataset/','CSV_Data/','.csv')\n",
    "upload_file_to_s3('../pdf-extractions/pypdf','PyPDF/','.txt')\n",
    "upload_file_to_s3('../pdf-extractions/grobid','Grobid/','.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading metadata of text files to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import datetime as dt\n",
    "import mimetypes\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions to create snowflake objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_internal_stage(engine, stage_name):\n",
    "    create_stage_query = f\"\"\"\n",
    "    CREATE OR REPLACE STAGE {stage_name};\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(create_stage_query))\n",
    "    print(\"Stage created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_data_into_stage(engine, csv_file_path, stage_name):\n",
    "    put_data_query = f\"\"\"\n",
    "    PUT file://{csv_file_path} @{stage_name};\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(put_data_query))\n",
    "    \n",
    "    print(\"File added to stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_with_csv_structure(engine, table_name):\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {table_name} (\n",
    "        file_name VARCHAR,\n",
    "        language VARCHAR,\n",
    "        version NUMBER,\n",
    "        encoding VARCHAR,\n",
    "        file_size NUMBER,\n",
    "        s3_url VARCHAR\n",
    "    );\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(create_table_query))\n",
    "    \n",
    "    print(f\"Table {table_name} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_stage_to_table(engine, table_name, stage_name):\n",
    "    copy_into_query = f\"\"\"\n",
    "    COPY INTO {table_name} FROM @{stage_name} FILE_FORMAT = (TYPE = CSV SKIP_HEADER = 1) ON_ERROR = 'CONTINUE';\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(copy_into_query))\n",
    "    \n",
    "    print(f\"Data Loaded from {stage_name} into table {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading metadata of grobid to snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to upload metadata to Snowflake table:  \n",
    "1. Create an engine for Snowflake connection\n",
    "2. Create an internal stage in Snowflake  \n",
    "3. Create a table with reference to CSV file structure  \n",
    "4. Adding file into stage  \n",
    "5. Load data from stage to table using COPY INTO command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage created\n",
      "File added to stage\n",
      "Table grobid_metadata created\n",
      "Data Loaded from grobid_metadata_stage into table grobid_metadata\n"
     ]
    }
   ],
   "source": [
    "# Creating SQLAlchemy connection and engine for snowflake\n",
    "user, password, account, warehouse, database, schema = snowflake_connection()\n",
    "sql_engine = create_engine(f'snowflake://{user}:{password}@{account}/?warehouse={warehouse}&database={database}&schema={schema}')\n",
    "\n",
    "# creating internal stage\n",
    "create_internal_stage(sql_engine, 'grobid_metadata_stage')\n",
    "\n",
    "# Adding file to stage\n",
    "put_data_into_stage(sql_engine, './metadata-grobid.csv', 'grobid_metadata_stage')\n",
    "\n",
    "# creating table into snowflake\n",
    "create_table_with_csv_structure(sql_engine, 'grobid_metadata')\n",
    "\n",
    "# loading data from stage to table\n",
    "load_data_from_stage_to_table(sql_engine, 'grobid_metadata','grobid_metadata_stage')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
